# ğŸ“˜ Awesome Safe Reinforcement Learning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of resources on **Safe Reinforcement Learning (Safe RL)** â€“ including papers, code, benchmarks, tutorials, and articles.  
Safe RL focuses on ensuring safety during training and deployment, by balancing reward optimization with constraints like risk, safety, or robustness.  

---

## ğŸ“‘ Table of Contents
- [Introduction](#introduction)  
- [Papers](#papers)  
- [Code & Frameworks](#code--frameworks)  
- [Benchmarks & Environments](#benchmarks--environments)  
- [Tutorials & Courses](#tutorials--courses)  
- [Articles & Blogs](#articles--blogs)  
- [Applications](#applications)  
- [Contributing](#contributing)  
- [License](#license)  

---

## ğŸ”¹ Introduction
Safe RL is critical for deploying reinforcement learning in the real world â€“ in **autonomous driving, robotics, drones, finance, and healthcare** â€“ where unsafe exploration can cause irreversible damage.  

This list compiles **research papers, implementations, and learning resources** to accelerate work in the field.  

---

## ğŸ“„ Papers
### Foundational
- **Constrained Policy Optimization (CPO)** â€“ Achiam et al., 2017.  
- **Lyapunov-based Safe RL** â€“ Chow et al., 2018.  
- **Constrained MDPs & Safe Exploration** â€“ Altman, 1999 (classic reference).  

### Recent & Notable
- **Safe Policy Optimization with Local Generalization** â€“ NeurIPS 2023.  
- **Safety Gymnasium: Benchmarking Safe Exploration** â€“ NeurIPS 2023.  
- **OmniSafe: Infrastructure for Safe RL** â€“ ICML 2023.  

*(expand with year-sorted list later)*  

---

## ğŸ’» Code & Frameworks
- [OmniSafe](https://github.com/PKU-Alignment/omnisafe) â€“ PyTorch-based Safe RL framework with multiple algorithms.  
- [FSRL](https://github.com/PKU-Alignment/fsrl) â€“ Safe RL in Tianshou, lightweight and practical.  
- [Safe-RL-Baselines](https://github.com/chenxianyu10/Safe-RL-Baselines) â€“ Implementations of Safe RL algorithms.  
- [Safety Starter Agents](https://github.com/openai/safety-starter-agents) â€“ OpenAI baseline implementations.  

---

## ğŸ§ª Benchmarks & Environments
- [Safety Gymnasium](https://github.com/PKU-Alignment/safety-gymnasium) â€“ successor to OpenAIâ€™s Safety Gym.  
- [Safe Control Gym](https://github.com/utiasDSL/safe-control-gym) â€“ safe control benchmarks in robotics.  
- [Safe Policy Optimization Benchmark](https://github.com/PKU-Alignment/safe-policy-optimization) â€“ NeurIPS 2023 benchmark suite.  
- [OpenAI Safety Gym](https://github.com/openai/safety-gym) â€“ early environment for safe exploration.  

---

## ğŸ“š Tutorials & Courses
- [Safe RL Course â€“ Topics in RL (PKU)](https://saferl-team.github.io/) â€“ lectures + curated papers.  
- [Safe RL Overview (Lilian Weng blog)](https://lilianweng.github.io/) â€“ accessible intro to Safe RL.  
- [Safe RL Workshop @ ICML/NeurIPS] â€“ slides and talks (link collections).  

---

## ğŸ“° Articles & Blogs
- **Why Safe RL Matters for Robotics** â€“ Medium (various posts).  
- **Safe RL in Healthcare Applications** â€“ arXiv insights.  
- **Towards Reliable RL Deployment** â€“ Berkeley AI blog.  

---

## ğŸ¤– Applications
- **Autonomous Driving** â€“ ensuring safety constraints in exploration.  
- **Robotics** â€“ preventing unsafe collisions during training.  
- **Healthcare** â€“ treatment policies with safety guarantees.  
- **Finance** â€“ risk-aware trading.  

---

## ğŸ¤ Contributing
Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) before submitting a pull request.  
We welcome:
- New papers with short summaries  
- Benchmarks and environments  
- Blog posts and tutorials  
- Practical implementations  

---

## ğŸ“œ License
[![CC0](https://licensebuttons.net/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/)  
